{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark galvenās datu struktūras un darbību izpilde \n",
    "\n",
    "Spark veic darbības ar atmiņā esošiem datiem aptuveni divas kārtas ātrāk, nekā mapreduce un aptuveni kārtu ātrāk ar datiem, kas atrodas uz diska.  \n",
    "\n",
    "Spark ir general purpose rīks, kas ir piemērots lielākajai daļai lielo datu uzdevumu.  \n",
    "\n",
    "Spark katra komanda ir **transformācija** vai **darbība**.  \n",
    "'Slinkā' novērtēšana atliek datu transformāciju izpildi līdz tā ir patiesi nepieciešama,ar katru nākamo transformāciju tikai papildinot/uzlabojot izpildes plānu. Kad tiek izsaukta darbība, transformācijas tiek izpildītas pēc plāna un rezultāts tiek vai nu atgriezts uz ekrāna, ierakstīts faiā vai failu sistēmā vai padots tālāk citām transformācijām.  \n",
    "\n",
    "\n",
    "Spark ir trīs veidu datu struktūras.  \n",
    "* Resilient Distributed Dataset (RDD)\n",
    "* Data Frame (DF)  \n",
    "* Data Set (DS)\n",
    "\n",
    "RDD ir Spark pamats. DF un DS struktūras ir izveidotas, uzbūvējot papildu loģiku un funkcionalitāti pa virsu RDD.  \n",
    "\n",
    "DF ir uzbūvēts, lai atļautu tipiskās tabulāro datu darbības. Līdzīgi kā pandas DataFrame, atbalsta filter, select utml. operācijas. Atšķirībā no pandas DataFrame, ja DF reģistrē (darbība), tad var var rakstīt SQL vaicājumus tieši uz DF. DF ir kolonnas un kolonnu tipi, bet viņi nav stingri definēti. Int kolonnā var ierakstīt string vērtības u.t.t.  \n",
    "\n",
    "DS ir DataFrame, tikai ar stingri definētiem kolonnu tipiem, kas atļauj nišejiskas optimizācijas un atļauj nelegālas darbības ar datiem (piem. string kolonnas vērtību atņemšana no integer kolonnas vērtībām) noķert, jau veidojot transformācijas (compile time), nevis, kad tiek izsaukta darbība (runtime).\n",
    "\n",
    "\n",
    "Pēc darbības izpildes, izpildes plānu un laiku, kas pavadīts uz katru darbību var redzēt caur Spark Web UI. Tas atļauj atrast algoritmiskos pudeles kaklus un tādējādi arī tos novērst.\n",
    "\n",
    "\n",
    "Ja tiek izmantoti vairākas savstarpēji neatkarīgas mašīnas, tad Spark katrai no šīm mašīnām piešķir skaitļošanas 'lomas'. Spark izmanto tā saucamo Driver/Executors pieeju. Serveris uz kura rezultāts tiek pieprasīts un atgriezts izpilda Driver lomu un visas pārējās mašīnas izpilda Executor lomu. Uz Driver tiek sastādīts izpildes plāns, un, ja uz Driver mašīnas atrodas dati, tad tie arī tiek nosūtīti visām pārējām mašīnām, uz kurām tad attiecīgi notiek paralelizējamie aprēķini.  \n",
    "Kopsavilkuma aprēķins un rezultātu atainošana gan notiek uz Driver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSession, tā konfigurēšana  \n",
    "SparkSession kalpo kā ieejas punkts visām datu struktūrām. Uz vienas mašīnas/klastera paralēli var eksistēt tikai viens SparkSession objekts un SparkSession objekts ir arī tieši saistīts ar Web UI.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-47-0.eu-north-1.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbc78848350>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jautājums: Vai šīs sekojošās koda rindiņas nostrādās?\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark datu struktūru izveide\n",
    "RDD var izveidot no Python datu struktūrām, bet lielākoties, ja mēs izmantojam Spark, tad datu avots būs vai nu HDFS/S3 vai lieli lokālie teksta faili.  \n",
    "RDD ir **nemaināma** (Immutable) datu struktūra. Ja uz to grib veikt darbības, jāveido jauna instance.  \n",
    "Tāpēc, ka RDD ir nemaināma, arī DF un DS ir nemaināmas. Nemaināmībai ir savas priekšrocības, kā vienu var minēt, ka mainīgais/datu struktūra var rasties tieši no vienas vietas, kas samazina potenciālos kļūdu avotus. Ja ir vairākas funkcijas, kas lasa vienu un to pašu datu struktūru un veic vienas un tās pašas darbības, rezultāts vienmēr būs viens un tas pats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 30, 40, 50]\n"
     ]
    }
   ],
   "source": [
    "# Python visas iebūvētās datu struktūras ir maināmas\n",
    "n = 2\n",
    "n+=1 # Mainīgajā n tiek mainīta vērtība 'in-place'. \n",
    "\n",
    "some_list = [20,30,40,50,7694]\n",
    "some_list.pop() # list arī ir 'in-place' maināma datu struktūra\n",
    "print(some_list)\n",
    "\n",
    "# Ja mēs .pop() lietotu vairākas reizes uz vienu un to pašu sarakstu, tad katru reizi mums būtu cits rezultāts.\n",
    "# Paradigma, kas to nepieļauj piespiež veidot jaunu struktūru, \n",
    "# tādējādi mums nebūs kļūdu, kas dažreiz notiek, bet citas reizes ne. Vismaz tieši Spark kodā."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD var izveidot no list\n",
    "lst = [[14],[23],[32],[41]]\n",
    "a = spark.sparkContext.parallelize(lst)\n",
    "a_mapped = a.map(lambda x: 2*x).map(lambda x: x[0]) # map funkcija tranformē katru elementu\n",
    "\n",
    "# Uzdevums:\n",
    "# Paskatamies uz web interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_mapped.reduce(lambda summ, elem: summ + elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrame izveide un darbības"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD var arī pārvērst par DataFrame, bet ir jāuzdefinē shēma\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "column_list = [StructField('kolonnas_nosauk',IntegerType(),True)]\n",
    "\n",
    "schema = StructType(column_list)\n",
    "\n",
    "b = a.toDF(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.createOrReplaceTempView(\"tabula\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jautājums: \n",
    "# Vai šis nostrādās?\n",
    "# spark.sql(\"select * from tabula\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_frame.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF var izveidot arī no csv faila\n",
    "csv_frame = spark.read.csv('order.csv')\n",
    "csv_frame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vai json faila\n",
    "json_frame = spark.read.json('example.json')\n",
    "json_frame.printSchema()\n",
    "# json_frame.show() # Šis nenostrādās jo json queryot tiešā veidā nav iespējams, ja nav json mapējuma uz shēmu\n",
    "# , bet ielasīšana notiek. \n",
    "# .show() efektīvi ir select * from df limit 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF darbības, izmantojot select, filter, map, reduce. arī show.\n",
    "# csv_frame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+-----------+\n",
      "|order_id|order_date| amount|customer_id|\n",
      "+--------+----------+-------+-----------+\n",
      "|       1|07/04/1776|$234.56|          1|\n",
      "|       2|03/14/1760| $78.50|          3|\n",
      "|       3|05/23/1784|$124.00|          2|\n",
      "|       4|09/03/1790| $65.50|          3|\n",
      "|       5|07/21/1795| $25.50|         10|\n",
      "|       6|11/27/1787| $14.40|          9|\n",
      "+--------+----------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_frame = spark.read.format(\"csv\").option(\"header\",\"true\").load('order.csv')\n",
    "csv_frame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tāpat kā pandas df, var lietot .describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uzdevums:\n",
    "# Ielasiet iekšā Spark DF datu failu, kas atrodas šeit:\n",
    "# https://raw.githubusercontent.com/reversedego/RCS_course/master/5_spark/records.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+-------------+--------------+----------+---------+----------+----------+----------+---------+-------------+----------+------------+\n",
      "|              Region|             Country|      Item Type|Sales Channel|Order Priority|Order Date| Order ID| Ship Date|Units Sold|Unit Price|Unit Cost|Total Revenue|Total Cost|Total Profit|\n",
      "+--------------------+--------------------+---------------+-------------+--------------+----------+---------+----------+----------+----------+---------+-------------+----------+------------+\n",
      "|  Sub-Saharan Africa|                Chad|Office Supplies|       Online|             L| 1/27/2011|292494523| 2/12/2011|      4484|    651.21|   524.96|   2920025.64|2353920.64|   566105.00|\n",
      "|              Europe|              Latvia|      Beverages|       Online|             C|12/28/2015|361825549| 1/23/2016|      1075|     47.45|    31.79|     51008.75|  34174.25|    16834.50|\n",
      "|Middle East and N...|            Pakistan|     Vegetables|      Offline|             C| 1/13/2011|141515767|  2/1/2011|      6515|    154.06|    90.93|   1003700.90| 592408.95|   411291.95|\n",
      "|  Sub-Saharan Africa|Democratic Republ...|      Household|       Online|             C| 9/11/2012|500364005| 10/6/2012|      7683|    668.27|   502.54|   5134318.41|3861014.82|  1273303.59|\n",
      "|              Europe|      Czech Republic|      Beverages|       Online|             C|10/27/2015|127481591| 12/5/2015|      3491|     47.45|    31.79|    165647.95| 110978.89|    54669.06|\n",
      "|  Sub-Saharan Africa|        South Africa|      Beverages|      Offline|             H| 7/10/2012|482292354| 8/21/2012|      9880|     47.45|    31.79|    468806.00| 314085.20|   154720.80|\n",
      "|                Asia|                Laos|     Vegetables|       Online|             L| 2/20/2011|844532620| 3/20/2011|      4825|    154.06|    90.93|    743339.50| 438737.25|   304602.25|\n",
      "|                Asia|               China|      Baby Food|       Online|             C| 4/10/2017|564251220| 5/12/2017|      3330|    255.28|   159.42|    850082.40| 530868.60|   319213.80|\n",
      "|  Sub-Saharan Africa|             Eritrea|           Meat|       Online|             L|11/21/2014|411809480| 1/10/2015|      2431|    421.89|   364.69|   1025614.59| 886561.39|   139053.20|\n",
      "|Central America a...|               Haiti|Office Supplies|       Online|             C|  7/4/2015|327881228| 7/20/2015|      6197|    651.21|   524.96|   4035548.37|3253177.12|   782371.25|\n",
      "|  Sub-Saharan Africa|              Zambia|         Cereal|      Offline|             M| 7/26/2016|773452794| 8/24/2016|       724|    205.70|   117.11|    148926.80|  84787.64|    64139.16|\n",
      "|              Europe|Bosnia and Herzeg...|      Baby Food|      Offline|             M|10/20/2012|479823005|11/15/2012|      9145|    255.28|   159.42|   2334535.60|1457895.90|   876639.70|\n",
      "|              Europe|             Germany|Office Supplies|       Online|             C| 2/22/2015|498603188| 2/27/2015|      6618|    651.21|   524.96|   4309707.78|3474185.28|   835522.50|\n",
      "|                Asia|               India|      Household|       Online|             C| 8/27/2016|151717174|  9/2/2016|      5338|    668.27|   502.54|   3567225.26|2682558.52|   884666.74|\n",
      "|Middle East and N...|             Algeria|        Clothes|      Offline|             C| 6/21/2011|181401288| 7/21/2011|      9527|    109.28|    35.84|   1041110.56| 341447.68|   699662.88|\n",
      "|Australia and Oce...|               Palau|         Snacks|      Offline|             L| 9/19/2013|500204360| 10/4/2013|       441|    152.58|    97.44|     67287.78|  42971.04|    24316.74|\n",
      "|Central America a...|                Cuba|      Beverages|       Online|             H|11/15/2015|640987718|11/30/2015|      1365|     47.45|    31.79|     64769.25|  43393.35|    21375.90|\n",
      "|              Europe|        Vatican City|      Beverages|       Online|             L|  4/6/2015|206925189| 4/27/2015|      2617|     47.45|    31.79|    124176.65|  83194.43|    40982.22|\n",
      "|Middle East and N...|             Lebanon|  Personal Care|      Offline|             H| 4/12/2010|221503102| 5/19/2010|      6545|     81.73|    56.67|    534922.85| 370905.15|   164017.70|\n",
      "|              Europe|           Lithuania|         Snacks|      Offline|             H| 9/26/2011|878520286| 10/2/2011|      2530|    152.58|    97.44|    386027.40| 246523.20|   139504.20|\n",
      "+--------------------+--------------------+---------------+-------------+--------------+----------+---------+----------+----------+----------+---------+-------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_frame = spark.read.format(\"csv\").option(\"header\",\"true\").load('records.csv')\n",
    "csv_frame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_frame.createOrReplaceTempView(\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"select * from 'Ship Date'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uzdevums:\n",
    "# 1. Uzrakstiet vaicājumu, kas saskaita cik ir unikālu valstu uz kurām tiek veiktas piegādes. Apskatiet izpildes plānu.\n",
    "# 2. Uzrakstiet vaicājumu, kas saskaita cik ir piegāžu uz katru valsti. \n",
    "# Salīdziniet izpildes plānu ar iepriekšējo punktu. \n",
    "# Vai kādas daļas tika izlaistas? Kāpēc tā varētu būt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Piemērs:\n",
    "# Vēlamies atlasīt pilnīgi visus decembra sūtījumus un pārbaudīt vai 12. mēnesī ienākumu mums bija vairāk, nekā vidēji.\n",
    "# Lai to izdarītu, nepieciešams pārveidot kolonnu 'Ship Date' par divām. Par dienu un mēnesi t.i. Ship_day , Ship_month\n",
    "\n",
    "from pyspark.sql.functions import col # col funkcija atļauj uz kolonnām atsaukties tikai ar to nosaukumiem\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "# regexp_replace un regexp_extract ir funkcijas, kas izmanto regular expressions teksta pavedienu šķelšanas principus. \n",
    "# Regular expressions ir ārkārtīgi plaši pielietota teksta pavedienu apstrādes metode, \n",
    "# sastopama visās programmēšanas valodās, bet tā nav pilnīgi universāla. Dažām valodām ir nelielas variācijas regex \n",
    "# simbolu mijiedarbībām.\n",
    "\n",
    "updated_frame = csv_frame\\\n",
    ".withColumn(\"Ship_day\",regexp_replace(col(\"Ship Date\"),\"\\/.*$\",\"\" ))\\\n",
    ".withColumn(\"Ship_month\",regexp_extract(col(\"Ship Date\"),\"(?<=\\/).*?(?=\\/)\",0))\\\n",
    ".withColumn(\"Revenue\",col(\"Total Revenue\")) # Pārsaucu kolonnu, lai nevajadzētu SQL vaicājumā lietot atstarpi\n",
    "\n",
    "# Šo var izdarīt arī ar MySQL ar funkciju, kas saucās tāpat.\n",
    "# https://dev.mysql.com/doc/refman/8.0/en/regexp.html\n",
    "\n",
    "# updated_frame.select([\"Ship_day\",\"Ship_month\"]).show() # Iebūvēto transformāciju piemērs, kur selectē sarakstu ar kolonnām.\n",
    "updated_frame.createOrReplaceTempView(\"sales\") # Padaram updated_frame pieejamu priekš SQL vaicājumiem kā tabulu 'sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------------+\n",
      "|Ship_month|sum(CAST(Revenue AS DOUBLE))|\n",
      "+----------+----------------------------+\n",
      "|         7|               4.056610547E8|\n",
      "|        15|        3.9870536796000034E8|\n",
      "|        11|        4.5867255731000006E8|\n",
      "|        29|         3.786441625999999E8|\n",
      "|         3|         4.521581724900002E8|\n",
      "|        30|         3.847275025599999E8|\n",
      "|         8|        4.2590097509999996E8|\n",
      "|        22|         4.533163571899999E8|\n",
      "|        28|         4.588998300499998E8|\n",
      "|        16|               4.035133133E8|\n",
      "|         5|        4.7138266552000004E8|\n",
      "|        31|        2.7400352576000005E8|\n",
      "|        18|         4.290000913999998E8|\n",
      "|        27|        5.1839069140000033E8|\n",
      "|        17|        4.2104703061999977E8|\n",
      "|        26|         5.008667586800002E8|\n",
      "|         6|         4.512693841599997E8|\n",
      "|        19|        4.2144749794000006E8|\n",
      "|        23|              4.4774310822E8|\n",
      "|        25|         4.021502548800001E8|\n",
      "+----------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ar šo vaicājumu tagad ir iegūts starprezultāts un tas parādīts uz ekrāna. \n",
    "spark.sql(\"select Ship_month, sum(Revenue) from sales group by ship_month\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uzdevumi: \n",
    "# 1. Novietojiet blakus 12. mēneša ienākumus un vidējos ienākumus pa visiem pārējiem mēnešiem.\n",
    "# 2. Apskatiet izpildes plānu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lai gan šajos piemēros lietots tiek SQL jo to ir intuitīvāk saprast un vieglāk nepieļaut sintaktiskas kļūdas, vienmēr var izmantot DataFrame datu struktūras transformācijas kā .select() , .orderBy() , .groupBy()  , .filter()  u.t.t.  \n",
    "Performances ziņā, starpības nav jo pēc tam, kad kods tiek noparsēts (īsi sakot, izveidots izpildes plāns), tas izpildās vienādi gan ķēdējot kopā funkcijas, gan ar spark.sql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uzdevums:\n",
    "# Saskaitiet visas piegādes, kuras netika veiktas uz valstīm, kuru nosaukumos nav e burta (mazā), \n",
    "# atrodot fundamentālo (nevis sintakses) kļūdu šajā kodā.\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "contains_frame = csv_frame\\\n",
    ".withColumn(\"does_contain\",lit(col(\"Country\").contains(\"e\")))\\\n",
    ".filter(csv_frame['does_contain'] != True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funkcionālās darbības  \n",
    "Šīs nav vienīgās darbības, kuras ar Spark datu struktūrām var veikt. Ir iespējams veikt arī programmistiskākas darbības, izmantojot .map() funkciju, bet tās strādā tikai uz RDD un DS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = [[\"Twinkle twinkle little star\"],\\\n",
    "          [\"How I wonder what you are\"],\\\n",
    "          [\"Twinkle twinkle little star\"]\\\n",
    "         ]\n",
    "words = spark.sparkContext.parallelize(lyrics) #Izveido RDD\n",
    "\n",
    "\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Definē shēmu, izmantojot Spark lietotos iebūvētos tipus. \n",
    "# # StructField - Lauks. Jeb kolonna.\n",
    "# Spark DF atbalsta arī alternatīvas kolonnu definīcijas.\n",
    "# Tāpēc arī jānorāda specifiski, ka kolonna 'Line' sastāvēs no string vērtībām un būs NULL`ojama.\n",
    "\n",
    "#StructType objektā ieliekam python sarakstu ar StructField objektiem.\n",
    "words_schema = StructType([StructField('Line',StringType(),True)])\n",
    "words_df = spark.createDataFrame(words,words_schema)\n",
    "\n",
    "# Piezīme sev: Pārbaudīt vai var uztaisīt trīs līmeņus ar hierarhiju un uztaisīt kkādu ORM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('twinkle', 2),\n",
       " ('wonder', 1),\n",
       " ('are', 1),\n",
       " ('Twinkle', 2),\n",
       " ('little', 2),\n",
       " ('star', 2),\n",
       " ('How', 1),\n",
       " ('I', 1),\n",
       " ('what', 1),\n",
       " ('you', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Klasiskais Spark wordcount piemērs\n",
    "# Katra 'rindiņa' iekš RDD ir saraksts, kurā iekšā ir string.\n",
    "# katrs ieraksts -  vārds tiek pārvērsts par ierakstu ('vards', 1)\n",
    "words.flatMap(lambda line : line[0].split(\" \")).map(lambda word: (word,1)).reduceByKey(lambda a,b: a+b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD transformācijas tipiski tiek izmantotas priekš datiem, kuriem nav definēta vai definējama shēma (visbiežāk teksts), bet strādājot ar datiem ar shēmu visbiežāk tiek izmantots dataframe.  \n",
    "Praktiski, jūs vienmēr ielasīsiet tabulārus datus un tad domāsiet, ko ar viņiem tālāk darīt.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uzdevums:  \n",
    "# 1) Ielasiet Spark DataFrame objektā json failu, kas atrodams šeit:\n",
    "\n",
    "# 2) Papildiniet šo DataFrame ar vēl divām rindām un saglabājiet to json failā flat_output.json\n",
    "# 3) Izpētiet rezultātu caur komandrindu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark skaitļošanas paralelizācija  \n",
    "Spark, kā neatkarīgo vienību uz kuras var veikt neatkarīgas darbības, sauc par partīciju. Partīciju skaits var noteikt skaitļošanas laiku. Spark datu struktūrām ir iespējams manuāli iestatīt to, cik partīcijās datus sadalīt.  \n",
    "Šo īpašību var izmantot, ja ir kādas divas, lielas, distributētas tabulas, piem. uz HDFS, tad, ja tabulas ielasot, mēs manuāli iestādam abām tabulām partīciju skaitu uz 10, tad, veicot joinu, notiek map side join  \n",
    "https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-joins-broadcast.html  \n",
    "To var darīt ar .repartition() vai .coalesce() funkcijām  \n",
    "\n",
    "\n",
    "Vēl viens veids kā izmantot partīciju skaitu, lai optimizētu būtu izmantot partīciju sapludināšana (.coalesce()), lai novērstu pilnu shuffle piem. apvienojot datus, kuri nav tik lieli, bet shuffle paredzams iespaidīgs (3+ groupby statementi spark.sql() komandā), tad varētu būtu prātīgi samazināt partīciju skaitu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Šādi var ierakstīt json failu no dataframe. Ievērojiet coalesce komandu.\n",
    "words_df.coalesce(1).write.format('json').save('coalesced_res.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uzdevums: \n",
    "# 1) Ielasiet coalesced_res.json atpakaļ dataframe objektā, saskaitiet unikālās vērtības katrā kolonnā.\n",
    "# 2) Apskatiet izpildes plānu un salīdziniet ielasīšanas operācijas ar tām, kas tika veiktas no flat_json.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uzdevums:\n",
    "# Katrā valstī ir savs piegādes serviss un katram pasūtījumam ir piegādes prioritāte.\n",
    "# Vissenāk pasūtītās lietas būtu jāpiegādā pirmās.\n",
    "# Izveidot sarakstu ar visprioritārākajām lietām iekš C, H un M prioritātēm katrai valstij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labs demo par to, ko ar Spark var izdarīt:  \n",
    "https://www.youtube.com/watch?v=2Qj1b4TruKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
