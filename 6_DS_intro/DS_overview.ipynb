{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ievads datu zinātnē\n",
    "* Kas ir datu zinātne un ko dara datu zinātnieks?  \n",
    "* Kas ir machine learning un kas tai ir kopīgs ar datu zinātni?  \n",
    "* Vispārīgas ML problēmas risināšanas shēma  \n",
    "* Exploratory Data Analysis  \n",
    "* Vienkāršākie klasifikācijas modeļi  \n",
    "* Vienkāršākie regresijas modeļi  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kas ir datu zinātne un ko dara datu zinātnieks?  \n",
    "Īsi sakot, datu zinātne ir statistika ar datorzinātnes un zināšanu inženierijas elementiem.  \n",
    "Datu zinātnieks ikdienā atbild **cenšās atbildēt** uz jautājumiem un atrast datu pievienoto vērtību, izmantojot programmistisku piekļuvi datubāzēm un datu kopām, lai padarītu datus pielietojamus pētīšanai un statistiskai analīzei, **apgūst nepieciešamās zināšanas** (domain knowledge), lai rezultātus interpretētu, **atrod un pielieto nepieciešamās metodes** un algoritmus, lai šos datus pētītu un **vizualizē/prezentē** iegūtos ieskatus vai rezultātus, **lai nonāktu pie kādas vēlamas rīcības**.  \n",
    "\n",
    "Dati ar kuriem datu zinātniekam ir jāstrādā, var atrasties relāciju / NoSQL datubāzē,    \n",
    "**nestrukturētos** (brīvas formas teksts, attēli, audio),  \n",
    "**daļēji strukturētos** (piem. JSON, kur values ir brīvas formas teksts, arī CSV skaitās daļēji strukturēti)  \n",
    "formātos uz serveriem vai datu ezerā, agregētā vai neagregētā formātā. Var arī būt, ka dati tiek straumēti, tādā gadījumā dati ir pieejami tikai caur kādu specifisku platformu vai rīku (piem. SparkStreaming) un ir jāmāk darboties arī šajā platformā, ja šos datus ir nepieciešams analizēt. Ar datu sākotnējo izpēti, pārveidošanu un padarīšanu pieejamu un ērtu analīzei vairāk nodarbojās datu inženieris, bet datu zinātnieks bieži vien arī veic šos pienākumus.  \n",
    "\n",
    "Lielākā mērogā, datu izpētes un analīzes komandā būtu jābūt gan datu inženieriem, gan datu zinātniekiem. Vidēji 2.5 inženieriem uz vienu zinātnieku."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kas ir Machine Learning un ar ko tas atšķirās no datu zinātnes?  \n",
    "Machine Learning (turpmāk ML) ir algoritmu kopa, kas izmanto statistikas un datorzinātnes konceptus, lai automatizēti veiktu specifiskus uzdevumus bez precīziem norādījumiem vai darbības pārraudzīšanas.  \n",
    "To lieto, lai automatizētu un paātrinātu atkārtojamas darbības vai arī atklātu kādas datiem piemītošas īpašības.  \n",
    "Piem. mēs daudz esam apskatījuši e-veikala piemēru, mēs varētu iegūt vairāk ienākumu, ja mēs klientiem varētu nosūtīt personalizētu preču katalogu ar lietām, kas klientam varētu interesēt, izejot no tā, ko viņš pirms tam ir pircis, piemēram, ar [tirgus groza jeb apriori algoritmu](https://en.wikipedia.org/wiki/Apriori_algorithm)\n",
    "\n",
    "\n",
    "Šis tipiski tiek panākts, izmantojot datus, lai piedzītu matemātiska modeļa parametrus un pēc tam, izmantojot izveidoto modeli, paredzētu ieraksta piederību kādai grupai vai kāda parametra skaitlisko vērtību iepriekš neredzētam ierakstam.  \n",
    "\n",
    "**ML ir tikai daļa no metodēm, ko datu zinātnieks var lietot, atbildot uz jautājumiem vai automatizējot kādas darbības**  \n",
    "\n",
    "Datu zinātniekam pieejamo metožu klāstā ir arī klasiskās statistiskās metodes (hipotēžu pārbaude, laikrindu analīze u.t.t.) un algoritmi, kas ir paredzēti datu dimensiju redukcijai, zināšanas par grafiku zīmēšanas bibliotēkām (it īpaši matplotlib), arī ar SQL, MS Excel, Power BI.  \n",
    "\n",
    "\n",
    "Machine Learning problēmas un tās atrisinošos algoritmus lielākoties iedala trīs (plus divās) lielās grupās.  \n",
    "Šīs trīs (plus dvas) grupas tiek iedalītas sekojoši:  \n",
    "![](https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2019/08/Types-of-Machine-Learning-algorithms.jpg)  \n",
    "Avots: https://data-flair.training/blogs/types-of-machine-learning-algorithms/  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality Reduction ir izdalāma atsevišķi jo  vairāk paredzēta pētniecībai un datu transformēšanai, nevis risina problēmu.  \n",
    "\n",
    "Reinforcement Learning ir reti pielietots un advancēts ML virziens, kas lielākoties izmanto mākslīgos neironu tīklus, lai risinātu ļoti sarežģītas mākslīgā intelekta problēmas.\n",
    "\n",
    "Trīs lielākās grupas raksturo divas pazīmes.  \n",
    "* Vai datu kopa ar kuru algoritms tiek piedzīts, **satur pareizo atbilžu kolonnu vai nē**, \n",
    "* Vai atbildē mums ir **nepieciešams iegūt skaitli** (Privātmājas sagaidāmā tirgus vērtība, lineāra modeļa slīpuma koeficients) **vai grupu** ('Riskants'/'Nav riskants' klients? Cik ir uzvedību raksturojošo grupu mūsu klientu kopā un kādas? Kurā grupā iekrīt mums interesējošs klients?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vispārīgas ML problēmas risināšanas shēma  \n",
    "ML problēmas risināšana sastāv no sekojošajiem soļiem:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Datu strukturēšana / pētīšana  \n",
    "Pirms mēs sākam datus pielietot modelī, tie būtu jāpārveido tabulārā vai vienkārši modelim saprotamā formātā un tie jāvizualizē. Jāiegūst pamatstatistikas par atsevišķām kolonnām / atslēgām, jāidentificē unikālo vērtību skaiti, jāidentificē kuras kolonnas / īpašības mēdz saturēt kļūdainu vai iztrūkstošu informāciju.  \n",
    "Šo dara arī datu inženieri, jo šis solis bieži vien ir visdarbietilpīgākais un nekad nav pilnībā pabeigts, ja vien nav iztestētas visas iespējamās uzvedības, kādas vien var parādīties datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Modeļa izvēle ![](https://scikit-learn.org/stable/_static/ml_map.png)  \n",
    "Lai gan ML modelis spēj pielāgot modeļa parametrus, pašu modeli ir jāizvēlās apzināti t.i. mēs neizvēlēsimies klasifikācijas algoritmu, ja nepieciešams automātiski prognozēt skaitlisku vērtību un neizvēlēsimies eksponenciālu sakarību modelēt ar lineāru modeli, pat ja ir iespējams salīdzinoši adekvātu slīpuma koeficientu.\n",
    "Avots: [oficiālā sklearn dokumentācija](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Datu attīrīšana / transformēšana  \n",
    "* Dati ne vienmēr ir piemēroti, lai tos pa taisno liktu iekšā modelī. Bieži var rasties situācijas, kad, piemēram, pandas, ielasot csv failu, kādu kolonnu interpretē kā string tipu. \n",
    "* Šajā solī tiek galā ar iztrūkstošajām vērtībām un vai nu tās speciāli apzīmē, iestāda default uzvedības vai arī kļūdainos ierakstus atmet.  \n",
    "* Var nākties arī n-kategoriskus teksta datus (vienā kolonnā ir n dažādas vērtības) izvērst par binārām vērtībām n kolonnās. Šī darbība ir pazīštama kā **One-Hot encoding**, kas ir plaši pielietots triks, lai tiktu galā ar kategoriskiem datiem.  \n",
    "* Datus varētu būt nepieciešams normēt vienā no vairākiem veidiem jo ir modeļi, kuri ir ļoti jūtīgi pret datu punktu absolūtajām vērtībām (SVM, PCA). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Datu kopas sadale  \n",
    "Lai piedzītu izvēlētā modeļa parametrus (to 'uztrenētu'), ir nepieciešams pietiekams daudzums datu. Pietiekamais daudzums variē no modeļa uz modeli, bet jo vairāk datu, jo labāka treniņkopa. \n",
    "Ja ir pieejama viena datu kopa un ir jāuztrenē modelis, mēs, protams, šo kopu izmantosim, lai trenētu modeli, bet, jāņem vērā, ka modeli mums pēc tam arī ir jāpārbauda.  \n",
    "Ja mums pieejamā kopa ir labelota ar pareizajām atbildēm (supervised learning), tad datu kopa ir jāsadala treniņa un testa (un iespējams arī validācijas) kopās.  \n",
    "\n",
    "**!! Turpmāk visi piemēri un analoģijas būs tieši ar supervised classification un supervised regression piemēriem. !!**  \n",
    "\n",
    "* Treniņa kopai būtu jābūt pietiekoši lielai, lai ierakstu (rindiņu) un īpašību (kolonnu) skaita attiecība būtu KĀ MINIMUMS 10:1 , bet praktiski tai būtu jābūt daudz lielākai.  \n",
    "* Testa/validācijas kopas arī nedrīkst būt pārāk mazas. sklearn iebūvētās funkcijas, treniņa/testa kopu veidošanai, testa kopā ievieto aptuveni 25% no visiem datiem.  \n",
    "* Validācijas kopa ir kopa ar ko tiek mērīta modeļa reālā uzvedība. Šī kopa ir jāizveido un to modelis trenēšanas-testēšanas ciklā nekad nedrīkst būt redzējis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Modeļa treniņš  \n",
    "Sekojot modeļa īpašībām un modeļa implementācijas dokumentācijai, modeli uztrenē.  \n",
    "Lielākajai daļai modeļu ir iestatīti noklusējuma parametri, bet tos visus var mainīt. Parametri, kas tiek padoti, modeli trenējot, ir tā saucamie hiperparametri t.i. tie nav modeļa aprēķināti, bet gan mūsu izvēlēti, modeļa uzvedību definējoši parametri. Hiperparameru piemērs ir piemēram K-NearestNeighbors tuvāko kaimiņu skaits, kas tiks lietots, modeli trenējot un lietojot.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Modeļa pārbaude  \n",
    "Kad modelis ir uztrenēts, laiks to pārbaudīt.  \n",
    "Ja mēs darbojamies ar labelotiem datiem, tad klasifikācijas problēmā mēs pārbaudītu gan cik procenti no visiem piemēriem mums atgriež pareizu labeli gan no treniņa kopas, gan no testa kopas.  \n",
    "\n",
    "Ar labelotām regresijas problēmām ir mazliet sarežģītāk. Var pārbaudīt R^2 vērtības, var pētīt vai residuals atbilst normālsadalījumam, bet, bieži vien, ja mēs darbojamies ar bibliotēkās paslēptām modeļu implementācijām, šīs metodes ir vispārinātas, pielāgotas un par tām jāuztraucās nav. Rezultāts tāpat ir skaitlis, kas raksturo modeļa paredzēšanas spējas robežās no 0 līdz 1.\n",
    "\n",
    "**! ! !** Bieži var saskarties ar to, ka treniņkopas precizitāte ir ārkārtīgi augsta (virs 90%), bet testa kopas, savukārt, ir zema. Tas nozīmē, ka modelis, tā vietā, lai būtu iemācījies kādu vispārīgu sakarību no datiem, ir vienkārši iegaumējis datu kopas specifiku. Šādu situāciju sauc par **over-fitting**. Šim par upuri bieži mēdz krist Decision Tree algoritms un tā variācijas.   \n",
    "\n",
    "Ja mēs esam uztrenējuši non-supervised modeli, tad tiešas testēšanas pieejas nav. Mēs varam veikt k-fold cross validation (izskaidrots nākamajā punktā) un pētīt vai modeļa precizitātes un izvade (parametru komplekts kā piem. iegūto klasteru skaits un īpašības) drastiski nemainās, varam mēģināt paši ar roku veikt kādu klasifikāciju, pētīt vai ir kāda loģika datiem sadalīties tieši šādi piem. socioloģiskos pētījumos varētu būt pamatojums tam, kāpēc konkrēti datu punkti iekrīt konkrētās grupās u.t.t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Train, test, analyze, repeat  \n",
    "Modeļa pirmās iterācijas reti kad atgriezīs apmierinošu precizitāti un saprast kāpēc precizitāte ir tāda, kāda ir, nav vienkāršs uzdevums.  \n",
    "Ir vairākas metodes, kas var palīdzēt sākt skatīties pareizajā virzienā.  \n",
    "* k-Fold Cross validation ir metode, kurā mēs paņemam visu datu kopu (izņemot validācijai paredzētos datus), nejauši sakārtojam un sadalam k daļās. Tad atkārtojam modeļa treniņu un testēšanu k reizes, katru reizi izmantojot citu daļu kā testa kopu un visas pārējās kā treniņkopu.  \n",
    "* Hiperparametru variēšana var palīdzēt uzlabot precizitāti. Tos arī varētu mēģināt variēt. \n",
    "* Var mēģināt izveido jaunu īpašību/kolonnu kā funkciju no kādas citas kolonnas un izmantot to vai nu papildus esošajām kolonnām, vai kā aizstājēju. Šīs kolonnas var vai nu sadalīt vienu kolonnu vairākās vai apvienot vairākas kolonnas vienā.  \n",
    "* Var pamēģināt arī tieši pretējo - atmest kādu kolonnu, kuras pienesums varētu būt minimāls vai ļoti trokšņains  \n",
    "\n",
    "Var pamēģināt arī darbības ar pašiem modeļiem:  \n",
    "* Var mēģināt ansambli ar modeļiem t.i. trenēt/lietot vairākus modeļus uz šiem datiem, kuri pēc tam 'vienojās' par pareizo atbildi, piem. izvelkot vidējo no visu regresijas modeļu sniegto paredzējumu summas.  \n",
    "* Var pamēģināt arī sadalīt datu kopu loģiskās daļās un izmēģināt dažādus modeļus dažādiem kādas kolonnas/-u vērtību intervāliem vai kategorijām.  \n",
    "\n",
    "Kad modelis ir uztrenēts un precizitāte ir apmierinoša, modelis ir jāpārbauda var validācijas kopu. Ja precizitāte vēl aizvien nav apmierinoša, būtu jāapsver cits modelis vai jāatrod variācija, kura strādā pietiekoši labi. \n",
    "Ja nekas nesanāk, jāatgriežās pie **0.** punkta un jāmēģina problēmu apskatīt savādāk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)  \n",
    "EDA ir plašs klāsts ar metodēm, sākot ar vienkāršu datu attēlošana, dažu kopsavilkumu aprēķināšanai un datu agregācijai līdz dimensiju redukcijai, dashboardu veidošanas praksēm, lai iegūtu kādus ieskatus no datiem.  \n",
    "Viena no populārākajām metodēm ir tā sauktā galveno komponenšu analīze - Principal Component Analysis (PCA).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nepieciešamās bibliotēkas un bibliotēku moduļi:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA # Galveno komponešu analīzes modulis\n",
    "from sklearn.model_selection import train_test_split # Kopu izveides modulis\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler # Modulis datu normēšanai\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Parauga datu kopas:\n",
    "from sklearn.datasets import load_iris\n",
    "iris_dataset = load_iris()\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30)\n",
      "(143, 30)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed shape: (426, 30)\n",
      "\n",
      "per-feature minimum before scaling:\n",
      " [6.981e+00 9.710e+00 4.379e+01 1.435e+02 5.263e-02 1.938e-02 0.000e+00\n",
      " 0.000e+00 1.060e-01 5.024e-02 1.153e-01 3.602e-01 7.570e-01 6.802e+00\n",
      " 1.713e-03 2.252e-03 0.000e+00 0.000e+00 9.539e-03 8.948e-04 7.930e+00\n",
      " 1.202e+01 5.041e+01 1.852e+02 7.117e-02 2.729e-02 0.000e+00 0.000e+00\n",
      " 1.566e-01 5.521e-02]\n",
      "\n",
      "per-feature maximum before scaling:\n",
      " [2.811e+01 3.928e+01 1.885e+02 2.501e+03 1.634e-01 2.867e-01 4.268e-01\n",
      " 2.012e-01 3.040e-01 9.575e-02 2.873e+00 4.885e+00 2.198e+01 5.422e+02\n",
      " 3.113e-02 1.354e-01 3.960e-01 5.279e-02 6.146e-02 2.984e-02 3.604e+01\n",
      " 4.954e+01 2.512e+02 4.254e+03 2.226e-01 9.379e-01 1.170e+00 2.910e-01\n",
      " 5.774e-01 1.486e-01]\n",
      "\n",
      "per-feature minimum after scaling:\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "per-feature maximum after scaling:\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "# print dataset properties before and after scaling\n",
    "print(\"transformed shape: {}\".format(X_train_scaled.shape))\n",
    "print(\"\")\n",
    "print(\"per-feature minimum before scaling:\\n {}\".format(X_train.min(axis=0)))\n",
    "print(\"\")\n",
    "print(\"per-feature maximum before scaling:\\n {}\".format(X_train.max(axis=0)))\n",
    "print(\"\")\n",
    "print(\"per-feature minimum after scaling:\\n {}\".format(X_train_scaled.min(axis=0)))\n",
    "print(\"\")\n",
    "print(\"per-feature maximum after scaling:\\n {}\".format(X_train_scaled.max(axis=0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per-feature minimum after scaling:\n",
      "[ 0.0336031   0.0226581   0.03144219  0.01141039  0.14128374  0.04406704\n",
      "  0.          0.          0.1540404  -0.00615249 -0.00137796  0.00594501\n",
      "  0.00430665  0.00079567  0.03919502  0.0112206   0.          0.\n",
      " -0.03191387  0.00664013  0.02660975  0.05810235  0.02031974  0.00943767\n",
      "  0.1094235   0.02637792  0.          0.         -0.00023764 -0.00182032]\n",
      "\n",
      "per-feature maximum after scaling:\n",
      "[0.9578778  0.81501522 0.95577362 0.89353128 0.81132075 1.21958701\n",
      " 0.87956888 0.9333996  0.93232323 1.0371347  0.42669616 0.49765736\n",
      " 0.44117231 0.28371044 0.48703131 0.73863671 0.76717172 0.62928585\n",
      " 1.33685792 0.39057253 0.89612238 0.79317697 0.84859804 0.74488793\n",
      " 0.9154725  1.13188961 1.07008547 0.92371134 1.20532319 1.63068851]\n"
     ]
    }
   ],
   "source": [
    "# transform test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# print test data properties after scaling\n",
    "print(\"per-feature minimum after scaling:\\n{}\".format(X_test_scaled.min(axis=0)))\n",
    "print(\"\")\n",
    "print(\"per-feature maximum after scaling:\\n{}\".format(X_test_scaled.max(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vienkāršākie klasifikācijas modeļi  \n",
    "Apskatīsim K-Nearest Neighbors un Logistic Regression algoritmus. Par spīti nosaukumam, Logistic Regression NAV regresijas modelis, bet klasifikācijas modelis, kas piemērots binārajai klasifikācijai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of iris_dataset:\n",
      " dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Keys of iris_dataset:\\n\", iris_dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, pre\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target names: ['setosa' 'versicolor' 'virginica']\n",
      "Feature names:\n",
      " ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Type of data: <class 'numpy.ndarray'>\n",
      "Shape of data: (150, 4)\n",
      "Type of target: <class 'numpy.ndarray'>\n",
      "Shape of target: (150,)\n",
      "First five rows of data:\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n"
     ]
    }
   ],
   "source": [
    "print(iris_dataset['DESCR'][:193] + \"\\n...\")\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"Target names:\", iris_dataset['target_names'])\n",
    "print(\"Feature names:\\n\", iris_dataset['feature_names'])\n",
    "print(\"Type of data:\", type(iris_dataset['data']))\n",
    "print(\"Shape of data:\", iris_dataset['data'].shape)\n",
    "print(\"Type of target:\", type(iris_dataset['target']))\n",
    "print(\"Shape of target:\", iris_dataset['target'].shape)\n",
    "print(\"First five rows of data:\\n\", iris_dataset['data'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (112, 4)\n",
      "y_train shape: (112,)\n",
      "X_test shape: (38, 4)\n",
      "y_test shape: (38,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "iris_dataset['data'], iris_dataset['target'], random_state=0)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe from data in X_train\n",
    "# label the columns using the strings in iris_dataset.feature_names\n",
    "iris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)\n",
    "# create a scatter matrix from the dataframe, color by y_train\n",
    "pd.plotting.scatter_matrix(iris_dataframe, c=y_train, figsize=(15, 15),\n",
    "                           marker='o', hist_kwds={'bins': 20}, s=60,\n",
    "                           alpha=.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# knn = KNeighborsClassifier(n_neighbors=1)\n",
    "# knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
